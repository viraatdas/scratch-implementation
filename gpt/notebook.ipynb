{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ac1efa-29e6-4a49-825a-5d1d820d687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "653177e0-aefe-4c48-84a5-d8d023d79d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T, 16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,16) @ (B, 16, T) --> (B,T,T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77fab46-e55d-44dc-8d0b-c43b4d68a9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1905, 0.8095, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3742, 0.0568, 0.5690, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1288, 0.3380, 0.1376, 0.3956, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4311, 0.0841, 0.0582, 0.3049, 0.1217, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0537, 0.3205, 0.0694, 0.2404, 0.2568, 0.0592, 0.0000, 0.0000],\n",
       "        [0.3396, 0.0149, 0.5165, 0.0180, 0.0658, 0.0080, 0.0373, 0.0000],\n",
       "        [0.0165, 0.0375, 0.0144, 0.1120, 0.0332, 0.4069, 0.3136, 0.0660]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcb97927-460f-4352-903c-242a011cd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor X shape: torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "# Random input tensor representing token embeddings\n",
    "X = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "print(\"Input tensor X shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee407ff5-4a43-4404-8c3a-b06fdc9eb575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Q shape: torch.Size([2, 4, 512])\n",
      "Projected K shape: torch.Size([2, 4, 512])\n",
      "Projected V shape: torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "query_layer = torch.nn.Linear(d_model, d_model)\n",
    "key_layer = torch.nn.Linear(d_model, d_model)\n",
    "value_layer = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "# Project input to Q, K, V\n",
    "Q = query_layer(X)  # Shape: (batch_size, seq_length, d_model)\n",
    "K = key_layer(X)    # Shape: (batch_size, seq_length, d_model)\n",
    "V = value_layer(X)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "print(\"Projected Q shape:\", Q.shape)\n",
    "print(\"Projected K shape:\", K.shape)\n",
    "print(\"Projected V shape:\", V.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6ee1f3-f372-4b36-bf69-da685a3c6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q split into heads shape: torch.Size([2, 8, 4, 64])\n",
      "K split into heads shape: torch.Size([2, 8, 4, 64])\n",
      "V split into heads shape: torch.Size([2, 8, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "Q = Q.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_length, d_k)\n",
    "K = K.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_length, d_k)\n",
    "V = V.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_length, d_k)\n",
    "\n",
    "print(\"Q split into heads shape:\", Q.shape)\n",
    "print(\"K split into heads shape:\", K.shape)\n",
    "print(\"V split into heads shape:\", V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "128a39a2-da8c-4380-bc5a-ddd900df967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttentioBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Embedding vector size. Each token in the input sequence is mapped \n",
    "        # to an embedding vector of the size\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // num_heads # dimension of vector seen by each head\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            # small value to indicate not to tend to those positions\n",
    "            attention_scores.masked_fil_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)            \n",
    "\n",
    "        return (attention_scores @ value)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # (batch. seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        x = MultiHeadAttentioBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "\n",
    "        # Combine the heads together\n",
    "        \n",
    "        # (batch, h, seq_len, d_k) --> () --> (batch, seq_len,d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b618766c-24f8-4084-9dcd-2ba3eee4ef20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiHeadAttentionBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a MultiHeadAttentionBlock instance\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m mha_block \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttentionBlock\u001b[49m(d_model, num_heads, dropout)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Random input tensor\u001b[39;00m\n\u001b[1;32m     10\u001b[0m q \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# Using the same tensor for query, key, and value\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiHeadAttentionBlock' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadAttentionBlock\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# Create a MultiHeadAttentionBlock instance\n",
    "mha_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "\n",
    "# Random input tensor\n",
    "q = k = v = x  # Using the same tensor for query, key, and value\n",
    "\n",
    "# Forward pass\n",
    "output = mha_block(q, k, v)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd214a-2b72-4207-bb22-99bf59f95ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
